<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mask6D Presentation - Stage 2</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            overflow: hidden;
        }

        .presentation-container {
            width: 100vw;
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        .slide {
            width: 90%;
            max-width: 1200px;
            height: 85vh;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            padding: 60px;
            display: none;
            flex-direction: column;
            position: relative;
            overflow-y: auto;
        }

        .slide.active {
            display: flex;
            animation: slideIn 0.5s ease-out;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateX(50px);
            }
            to {
                opacity: 1;
                transform: translateX(0);
            }
        }

        .slide-header {
            color: #1e3a8a;
            font-size: 2.5em;
            font-weight: bold;
            margin-bottom: 30px;
            border-bottom: 4px solid #f59e0b;
            padding-bottom: 15px;
        }

        .slide-content {
            flex: 1;
            font-size: 1.2em;
            line-height: 1.8;
            color: #1f2937;
        }

        .slide-content h3 {
            color: #1e3a8a;
            margin-top: 25px;
            margin-bottom: 15px;
            font-size: 1.5em;
        }

        .slide-content ul {
            margin-left: 30px;
            margin-top: 15px;
        }

        .slide-content li {
            margin-bottom: 15px;
            padding-left: 10px;
        }

        .highlight-box {
            background: #fef3c7;
            border-left: 5px solid #f59e0b;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }

        .contribution-box {
            background: #e0e7ff;
            border-left: 5px solid #4f46e5;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }

        .problem-box {
            background: #fee2e2;
            border-left: 5px solid #ef4444;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }

        .success-box {
            background: #d1fae5;
            border-left: 5px solid #10b981;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }

        .formula {
            background: #f3f4f6;
            padding: 15px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            text-align: center;
            margin: 20px 0;
            font-size: 1.1em;
        }

        .two-column {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 20px 0;
        }

        .navigation {
            position: fixed;
            bottom: 30px;
            right: 30px;
            display: flex;
            gap: 15px;
            z-index: 1000;
        }

        .nav-button {
            background: #1e3a8a;
            color: white;
            border: none;
            padding: 15px 30px;
            border-radius: 10px;
            cursor: pointer;
            font-size: 1.1em;
            transition: all 0.3s;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }

        .nav-button:hover {
            background: #2563eb;
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(0,0,0,0.3);
        }

        .nav-button:disabled {
            background: #9ca3af;
            cursor: not-allowed;
            transform: none;
        }

        .slide-counter {
            position: fixed;
            bottom: 30px;
            left: 30px;
            background: #1e3a8a;
            color: white;
            padding: 15px 25px;
            border-radius: 10px;
            font-size: 1.1em;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }

        .icon {
            font-size: 1.5em;
            margin-right: 10px;
        }

        .image-placeholder {
            background: #e5e7eb;
            border: 3px dashed #9ca3af;
            border-radius: 10px;
            padding: 40px;
            text-align: center;
            color: #6b7280;
            font-style: italic;
            margin: 20px 0;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        .comparison-table th,
        .comparison-table td {
            border: 2px solid #e5e7eb;
            padding: 15px;
            text-align: left;
        }

        .comparison-table th {
            background: #1e3a8a;
            color: white;
            font-weight: bold;
        }

        .comparison-table tr:nth-child(even) {
            background: #f9fafb;
        }

        .title-slide {
            justify-content: center;
            align-items: center;
            text-align: center;
        }

        .title-slide .slide-header {
            font-size: 3em;
            border: none;
            margin-bottom: 40px;
        }

        .subtitle {
            font-size: 1.5em;
            color: #6b7280;
            margin: 20px 0;
        }

        .authors {
            font-size: 1.3em;
            color: #1f2937;
            margin: 15px 0;
        }

        .conference {
            font-size: 1.2em;
            color: #f59e0b;
            font-weight: bold;
            margin: 15px 0;
        }

        .checkmark {
            color: #10b981;
            font-weight: bold;
        }

        .numbered-list {
            counter-reset: item;
            list-style: none;
            margin-left: 0;
        }

        .numbered-list li {
            counter-increment: item;
            margin-bottom: 20px;
            padding-left: 0;
        }

        .numbered-list li::before {
            content: counter(item) "️⃣ ";
            font-weight: bold;
            font-size: 1.3em;
            margin-right: 10px;
        }

        .emphasis {
            background: #fef3c7;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: bold;
            color: #92400e;
        }

        .speaker-notes {
            background: #f3f4f6;
            border-left: 4px solid #6b7280;
            padding: 15px;
            margin-top: 20px;
            font-size: 0.9em;
            color: #4b5563;
            font-style: italic;
        }

        @media print {
            .navigation, .slide-counter {
                display: none;
            }
            .slide {
                page-break-after: always;
                display: block !important;
                height: auto;
            }
        }
    </style>
</head>
<body>
    <div class="presentation-container">
        
        <!-- Slide 1: Title -->
        <div class="slide active title-slide">
            <div class="slide-header">
                MASK6D: MASKED POSE PRIORS FOR<br>6D OBJECT POSE ESTIMATION
            </div>
            <div class="authors">
                Yuechen Xie, Haobo Jiang, Jin Xie
            </div>
            <div class="subtitle">
                Nanjing University of Science and Technology
            </div>
            <div class="conference">
                ICASSP 2024
            </div>
            <div class="subtitle" style="margin-top: 40px;">
                Team 7 - Stage 2: Method Components & Contributions
            </div>
        </div>

        <!-- Slide 2: Problem Statement -->
        <div class="slide">
            <div class="slide-header">
                The Challenge: 6D Pose Estimation in Real Scenes
            </div>
            <div class="slide-content">
                <div class="image-placeholder">
                    <img src="Screenshot 2025-10-01 094942.png" alt="Flowers in Chan">
                </div>
                
                <h3>Three Core Problems:</h3>
                
                <div class="problem-box">
                    <strong>⚠️ 1. LIMITED POSE-AWARE FEATURES</strong>
                    <ul>
                        <li>2D CNNs struggle with occluded objects</li>
                        <li>Cannot extract meaningful features from limited visible regions</li>
                    </ul>
                </div>

                <div class="problem-box">
                    <strong>⚠️ 2. BACKGROUND INTERFERENCE</strong>
                    <ul>
                        <li>Cluttered scenes introduce significant noise</li>
                        <li>Degrades prediction accuracy</li>
                    </ul>
                </div>

                <div class="problem-box">
                    <strong>⚠️ 3. NO POSE PRIOR KNOWLEDGE</strong>
                    <ul>
                        <li>Cannot reason about hidden object parts</li>
                        <li>Lack spatial understanding of 3D geometry</li>
                    </ul>
                </div>
            </div>  
        </div>

        <!-- Slide 3: Core Innovation -->
        <div class="slide">
            <div class="slide-header">
                Core Innovation: Pose-Specific Multi-Modal Input
            </div>
            <div class="slide-content">
                <div class="image-placeholder" style="display: flex; justify-content: center;"  >
                    <img src="Screenshot 2025-10-01 095734.png" alt="Flowers in Chan">
                </div>

                <h3>Three Input Modalities:</h3>

                <div class="highlight-box">     
                    <strong>📷 RGB IMAGE</strong><br>
                    Standard visual appearance information
                </div>

                <div class="highlight-box">
                    <strong>🎯 2D-3D CORRESPONDENCE MAP</strong>
                    <ul>
                        <li>Projects 3D object model to 2D pixels</li>
                        <li>Directly encodes <span class="emphasis">POSE</span> information</li>
                        <li>Reflects object orientation in camera coordinates</li>
                    </ul>
                </div>

                <div class="highlight-box">
                    <strong>🎭 VISIBLE MASK MAP</strong>
                    <ul>
                        <li>Identifies object regions vs. background</li>
                        <li>Guides network to focus on object</li>
                    </ul>
                </div>

                <div class="success-box">
                    <strong>✨ KEY INSIGHT:</strong><br>
                    Unlike generic MAE (depth/segmentation), Mask6D uses <span class="emphasis">POSE-SPECIFIC</span> modalities
                </div>
            </div>
        </div>

        <!-- Slide 4: Pre-training Architecture -->
        <div class="slide">
            <div class="slide-header">
                Pre-Training Phase: Learning Pose Priors
            </div>
            <div class="slide-content">
                <div class="image-placeholder" style="display: flex; justify-content: center;"  >
                    <img src="Screenshot 2025-10-01 095734.png" alt="Flowers in Chan">
                </div>

                <h3>Pipeline Steps:</h3>

                <ul class="numbered-list">
                    <li><strong>PATCH TOKENIZATION</strong>
                        <ul style="list-style: disc; margin-left: 30px;">
                            <li>RGB: 16×16 patches (preserves semantics)</li>
                            <li>Mask/Correspondence: 4×4 patches (captures geometry)</li>
                            <li>Each modality → 256 patches</li>
                        </ul>
                    </li>
                    <li><strong>RANDOM MASKING</strong>
                        <ul style="list-style: disc; margin-left: 30px;">
                            <li>75% of patches masked</li>
                            <li>Simulates occlusion scenarios</li>
                        </ul>
                    </li>
                    <li><strong>ViT ENCODER</strong>
                        <ul style="list-style: disc; margin-left: 30px;">
                            <li>Processes only visible patches</li>
                            <li>Learns pose-aware representations</li>
                        </ul>
                    </li>
                    <li><strong>DECODER RECONSTRUCTION</strong>
                        <ul style="list-style: disc; margin-left: 30px;">
                            <li>Predicts masked regions</li>
                        </ul>
                    </li>
                </ul>

                <div class="contribution-box">
                    <strong>🎯 OBJECT-FOCUSED LOSS:</strong>
                    <div class="formula">
                        L<sub>FOCUS</sub> = L<sub>COR</sub> + L<sub>MASK</sub> + L<sub>RGB</sub>
                    </div>
                    <center><strong>Only supervises OBJECT pixels (ignores background)</strong></center>
                </div>
            </div>
            
        </div>

        <!-- Slide 5: Fine-tuning Architecture -->
        <div class="slide">
            <div class="slide-header">
                Fine-Tuning Phase: Adapting to Pose Estimation
            </div>
            <div class="slide-content">
                <div class="image-placeholder" style="display: flex; justify-content: center;"  >
                    <img src="Screenshot 2025-10-01 100646.png" alt="Flowers in Chan">
                </div>

                <div class="problem-box">
                    <strong>THE CHALLENGE:</strong><br>
                    At test time → Only RGB available<br>
                    (No correspondence map, no visible mask)
                </div>

                <h3>The Solution:</h3>

                <ul class="numbered-list">
                    <li><strong>MULTI-MODAL PREDICTION</strong>
                        <ul style="list-style: disc; margin-left: 30px;">
                            <li>Pre-trained encoder predicts correspondence + mask from RGB alone</li>
                        </ul>
                    </li>
                    <li><strong>ViT ADAPTER INTEGRATION</strong>
                        <ul style="list-style: disc; margin-left: 30px;">
                            <li>Adds inductive bias for dense prediction tasks</li>
                        </ul>
                    </li>
                    <li><strong>TRANS-PnP NETWORK</strong>
                        <ul style="list-style: disc; margin-left: 30px;">
                            <li>Reuses 3-5 pre-trained encoder blocks</li>
                            <li>Directly regresses 6D pose (R, t)</li>
                            <li>Blocks used: LM(3), LM-O(4), YCB-V(5)</li>
                        </ul>
                    </li>
                </ul>

                <div class="image-placeholder" style="display: flex; justify-content: center;"  >
                    <img src="Screenshot 2025-10-01 101303.png" alt="Flowers in Chan">
                </div>

                <div class="success-box">
                    <strong>Visual Proof:</strong> Pre-trained attention cleanly focuses on object even when truncated!
                </div>
            </div>
        </div>

        <!-- Slide 6: Technical Contributions -->
        <div class="slide">
            <div class="slide-header">
                Three Main Technical Contributions
            </div>
            <div class="slide-content">
                <div class="contribution-box">
                    <strong>🔬 1. NOVEL PRE-TRAINING FRAMEWORK</strong>
                    <ul>
                        <li>First <span class="emphasis">pose-specific</span> masked autoencoding approach</li>
                        <li>Learns implicit 3D geometry understanding</li>
                        <li>Masking simulates real occlusion scenarios</li>
                    </ul>
                </div>

                <div class="contribution-box">
                    <strong>🎯 2. POSE-AWARE MULTI-MODAL DESIGN</strong>
                    <ul>
                        <li><strong>2D-3D correspondence as pose supervision signal</strong>
                            <ul style="list-style: circle; margin-left: 30px;">
                                <li>Explicitly encodes spatial relationships</li>
                            </ul>
                        </li>
                        <li><strong>Visible mask suppresses background interference</strong>
                            <ul style="list-style: circle; margin-left: 30px;">
                                <li>Acts as attention mechanism</li>
                            </ul>
                        </li>
                        <li>Both essential for cluttered/occluded scenarios</li>
                    </ul>
                </div>

                <div class="contribution-box">
                    <strong>💡 3. OBJECT-FOCUSED SUPERVISION STRATEGY</strong>
                    <ul>
                        <li>Partial-pixel loss vs. full-pixel (standard MAE)</li>
                        <li>Mathematically eliminates background gradients</li>
                        <li>Enables pure object feature learning</li>
                    </ul>
                    <div class="formula">
                        L<sub>FOCUS</sub> = (L<sub>COR</sub> + L<sub>MASK</sub> + L<sub>RGB</sub>) ⊙ Mask
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 7: Complete Method Overview -->
        <div class="slide">
            <div class="slide-header">
                Complete Method Architecture
            </div>
            <div class="slide-content">
                <div class="two-column">
                    <div style="background: #e0e7ff; padding: 20px; border-radius: 10px;">
                        <h3 style="color: #4f46e5; margin-top: 0;">PRE-TRAINING PHASE</h3>
                        <strong>Input:</strong> RGB + 2D-3D Corr + Mask<br><br>
                        <strong>Process:</strong><br>
                        → Patch Masking (75%)<br>
                        → ViT Encoder<br>
                        → Decoder<br><br>
                        <strong>Loss:</strong> Object-Focused<br><br>
                        <strong>Output:</strong> Pre-trained Encoder with pose priors
                    </div>
                    <div style="background: #d1fae5; padding: 20px; border-radius: 10px;">
                        <h3 style="color: #10b981; margin-top: 0;">FINE-TUNING PHASE</h3>
                        <strong>Input:</strong> RGB only<br><br>
                        <strong>Process:</strong><br>
                        → Predict Corr + Mask<br>
                        → Trans-PnP Network<br>
                        → (3-5 encoder blocks)<br><br>
                        <strong>Output:</strong> 6D Pose (R, t)
                    </div>
                </div>

                <div class="image-placeholder" style="display: flex; justify-content: center;"  >
                    <img src="Screenshot 2025-10-01 101535.png" alt="Flowers in Chan">
                </div>

                <h3>Key Design Decisions:</h3>
                <ul>
                    <li><strong>400K pre-training steps</strong> for proper convergence</li>
                    <li><strong>Input resolution:</strong> 256×256 (RGB), 64×64 (geometric maps)</li>
                    <li><strong>Trans-PnP depth adapts to dataset difficulty</strong>
                        <ul style="list-style: circle; margin-left: 30px;">
                            <li>LM: 3 blocks (simpler)</li>
                            <li>LM-O: 4 blocks</li>
                            <li>YCB-V: 5 blocks (most challenging)</li>
                        </ul>
                    </li>
                    <li><strong>ViT Adapter</strong> for inductive bias</li>
                </ul>
            </div>
        </div>

        <!-- Slide 8: Why This Works -->
        <div class="slide">
            <div class="slide-header">
                Why Mask6D is Effective
            </div>
            <div class="slide-content">
                <h3>Success Factors:</h3>

                <div class="success-box">
                    <strong>🧩 GEOMETRIC LEARNING</strong><br>
                    Reconstructing 2D-3D correspondences teaches spatial consistency of 3D objects
                </div>

                <div class="success-box">
                    <strong>🎭 OCCLUSION SIMULATION</strong><br>
                    Random masking mimics diverse real-world occlusion patterns
                </div>

                <div class="success-box">
                    <strong>🚫 BACKGROUND SUPPRESSION</strong><br>
                    Mask + object-focused loss mathematically eliminate background interference
                </div>

                <div class="success-box">
                    <strong>🔄 PRIOR TRANSFER</strong><br>
                    Rich spatial understanding transfers seamlessly to downstream pose estimation
                </div>

                <h3 style="margin-top: 30px;">Comparison to Baselines:</h3>

                <table class="comparison-table">
                    <tr>
                        <th>Approach</th>
                        <th>Characteristics</th>
                    </tr>
                    <tr>
                        <td><strong>Traditional Methods</strong></td>
                        <td>Train from scratch, no pose priors</td>
                    </tr>
                    <tr>
                        <td><strong>Mask6D (Ours)</strong></td>
                        <td><span class="emphasis">Pre-trained 3D geometry understanding</span></td>
                    </tr>
                </table>

                <div class="image-placeholder" style="display: flex; justify-content: center;"  >
                    <img src="Screenshot 2025-10-01 101303.png" alt="Flowers in Chan">
                </div>
            </div>
        </div>

        <!-- Slide 9: Conclusion -->
        <div class="slide">
            <div class="slide-header">
                Conclusion & Impact
            </div>
            <div class="slide-content">
                <h3>Key Takeaways:</h3>

                <div class="success-box">
                    <span class="checkmark">✅</span> First pose-specific masked autoencoding framework
                </div>

                <div class="success-box">
                    <span class="checkmark">✅</span> Multi-modal design with 2D-3D correspondence and visible masks addresses core challenges
                </div>

                <div class="success-box">
                    <span class="checkmark">✅</span> Object-focused supervision eliminates background interference mathematically
                </div>

                <div class="success-box">
                    <span class="checkmark">✅</span> State-of-the-art performance on standard benchmarks (LM, LM-O, YCB-V)
                </div>

                <div class="success-box">
                    <span class="checkmark">✅</span> Demonstrates task-specific pre-training > generic pre-training
                </div>

                <div class="highlight-box" style="margin-top: 30px;">
                    <strong>💡 KEY INSIGHT:</strong><br><br>
                    Pre-training should be tailored to task-specific challenges. Learning to reconstruct pose-encoding correspondences builds exactly the priors needed for robust 6D pose estimation.
                </div>

                <div style="text-align: center; margin-top: 40px; font-size: 1.3em;">
                    <strong>IMPACT:</strong> Advances pose estimation for robotics, AR, and autonomous systems in cluttered real-world environments
                </div>

                <div style="text-align: center; margin-top: 50px; font-size: 2em; color: #1e3a8a; font-weight: bold;">
                    THANK YOU
                </div>
                <div style="text-align: center; font-size: 1.5em; color: #6b7280; margin-top: 20px;">
                    Questions?
                </div>
            </div>
        </div>

    </div>

    <!-- Navigation Controls -->
    <div class="navigation">
        <button class="nav-button" id="prevBtn" onclick="changeSlide(-1)">← Previous</button>
        <button class="nav-button" id="nextBtn" onclick="changeSlide(1)">Next →</button>
    </div>

    <!-- Slide Counter -->
    <div class="slide-counter">
        <span id="slideNumber">1</span> / <span id="totalSlides">9</span>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        
        document.getElementById('totalSlides').textContent = totalSlides;

        function showSlide(n) {
            slides[currentSlide].classList.remove('active');
            currentSlide = (n + totalSlides) % totalSlides;
            slides[currentSlide].classList.add('active');
            
            document.getElementById('slideNumber').textContent = currentSlide + 1;
            
            // Update button states
            document.getElementById('prevBtn').disabled = currentSlide === 0;
            document.getElementById('nextBtn').disabled = currentSlide === totalSlides - 1;
        }

        function changeSlide(direction) {
            showSlide(currentSlide + direction);
        }

        // Keyboard navigation
        document.addEventListener('keydown', function(event) {
            if (event.key === 'ArrowRight' || event.key === ' ') {
                if (currentSlide < totalSlides - 1) changeSlide(1);
            } else if (event.key === 'ArrowLeft') {
                if (currentSlide > 0) changeSlide(-1);
            }
        });

        // Initialize
        showSlide(0);
    </script>
</body>
</html>